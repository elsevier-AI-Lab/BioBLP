{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2545473",
   "metadata": {},
   "source": [
    "# Evaluating a trained link predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae53d26d",
   "metadata": {},
   "source": [
    "We will take a closer look at how a trained link predictor performs in specific cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044e748a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T15:29:25.789668Z",
     "start_time": "2023-04-20T15:29:25.768705Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os.path as osp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pykeen.evaluation import RankBasedEvaluator\n",
    "from pykeen.triples import TriplesFactory\n",
    "import torch\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e131c5cf",
   "metadata": {},
   "source": [
    "Change this cell to generate a report for other datasets/models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db315cb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T15:29:27.712727Z",
     "start_time": "2023-04-20T15:29:27.695239Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_ID = '36viovqn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62db008",
   "metadata": {},
   "source": [
    "## Loading model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cde739df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T15:15:58.570984Z",
     "start_time": "2023-04-20T15:15:55.735323Z"
    }
   },
   "outputs": [],
   "source": [
    "base_path = osp.join('..', 'models', MODEL_ID)\n",
    "model_path = osp.join(base_path, 'trained_model.pkl')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.load(model_path).to(device)\n",
    "train = TriplesFactory.from_path_binary(osp.join(base_path, 'training_triples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0b5f96a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T15:15:59.618349Z",
     "start_time": "2023-04-20T15:15:58.566325Z"
    }
   },
   "outputs": [],
   "source": [
    "graph_path = osp.join('..', 'data', 'biokgb', 'graph')\n",
    "valid_triples = 'biokg.links-valid.csv'\n",
    "test_triples = 'biokg.links-test.csv'\n",
    "\n",
    "valid, test = [TriplesFactory.from_path(osp.join(graph_path, f),\n",
    "                                        entity_to_id=train.entity_to_id,\n",
    "                                        relation_to_id=train.relation_to_id)\n",
    "               for f in (valid_triples, test_triples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1c764",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e93ceb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RankBasedEvaluator(filtered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f362096d",
   "metadata": {},
   "source": [
    "### Evaluation over the full set of relation types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2fd34d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T15:16:32.652529Z",
     "start_time": "2023-04-20T15:16:09.528598Z"
    }
   },
   "outputs": [],
   "source": [
    "results = evaluator.evaluate(model, test.mapped_triples,\n",
    "                             additional_filter_triples=[train.mapped_triples,\n",
    "                                                        valid.mapped_triples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e9ada",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T15:18:02.793839Z",
     "start_time": "2023-04-20T15:18:02.735388Z"
    }
   },
   "outputs": [],
   "source": [
    "results.get_metric('both.realistic.hits_at_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99e1860",
   "metadata": {},
   "source": [
    "### Evaluating over specific relation types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac0196",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T15:32:40.695244Z",
     "start_time": "2023-04-20T15:29:31.229851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbca2d2c98754aa8bd78ec70e58bc369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating over each relation:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d217dd0e43dd42a9af58f042e0ae212a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating on cuda:0:   0%|          | 0.00/878 [00:00<?, ?triple/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b3e04f4bf84e87a0d5a6e08b400942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating on cuda:0:   0%|          | 0.00/1.65k [00:00<?, ?triple/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b0682af6eb40f9b23e8a6727c89dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating on cuda:0:   0%|          | 0.00/121k [00:00<?, ?triple/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.relation_to_id\n",
    "result_dicts = []\n",
    "for relation in tqdm(train.relation_to_id, desc='Evaluating over each relation'):\n",
    "    triples_subset = test.new_with_restriction(relations=[relation])\n",
    "    if triples_subset.num_triples > 0:\n",
    "        subset_result = evaluator.evaluate(model,\n",
    "                                           triples_subset.mapped_triples,\n",
    "                                           additional_filter_triples=[train.mapped_triples,\n",
    "                                                                      valid.mapped_triples],\n",
    "                                           batch_size=16)\n",
    "        result_dicts.append({'results': subset_result, 'relation': relation, 'count': triples_subset.num_triples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e70c5c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T15:40:18.379146Z",
     "start_time": "2023-04-20T15:40:18.323846Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([{'relation': r['relation'], 'count': r['count'], **r['results'].to_flat_dict()} for r in result_dicts])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f21caf0",
   "metadata": {},
   "source": [
    "Here we save the results to a csv file, so we can load it later and make plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8ece4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T15:41:31.869495Z",
     "start_time": "2023-04-20T15:41:31.792866Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df.to_csv(osp.join(base_path, 'results_by_relation.csv'), index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b49a448",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84c39a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "restricted_rels_macro_performance = results_df[results_df.columns[2:]].mean(axis=0)\n",
    "restricted_rels_macro_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b74fe",
   "metadata": {},
   "source": [
    "Note that this is **not** the same as the original, unrestricted evaluation. When restricting by relation, the average above is a *macro-average*, where all relations are weighted equally. In the unrestricted scenario, we average over all triples, which is a *micro-average* where more frequent relations are weighted higher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c57d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all_rels_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452cc251",
   "metadata": {},
   "source": [
    "Since we have the triple counts for each relation, we can compute a micro-average instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1db805",
   "metadata": {},
   "outputs": [],
   "source": [
    "restricted_rels_micro_performance = results_df[results_df.columns[2:]].mul(results_df['Count'], axis=0).sum(axis=0) / results_df['Count'].sum()\n",
    "restricted_rels_micro_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679a5d94",
   "metadata": {},
   "source": [
    "How do MRR, H@k, and AMR correlate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3719bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_pair(results_df, metric_1: str, metric_2: str):\n",
    "    \"\"\"Make a scatter plot with one link prediction metric in each axis.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.scatter(results_df[metric_1], results_df[metric_2])\n",
    "    plt.xlabel(metric_1)\n",
    "    plt.ylabel(metric_2)\n",
    "\n",
    "plot_metric_pair(results_df, 'mean_reciprocal_rank', 'hits_at_10')\n",
    "plot_metric_pair(results_df, 'mean_reciprocal_rank', 'adjusted_mean_rank')\n",
    "plot_metric_pair(results_df, 'mean_reciprocal_rank', 'Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c852cf3",
   "metadata": {},
   "source": [
    "What are the relations where the model performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_relation_plot(results_df, metric: str):\n",
    "    \"\"\"Make a bar plot of link prediction performance for each relation.\"\"\"\n",
    "    results_df[['Relation', metric]].sort_values(by=metric).plot.barh(x='Relation', figsize=(5, 5), grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d3f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_relation_plot(results_df, 'hits_at_10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c473ec8",
   "metadata": {},
   "source": [
    "### Evaluating over specific entity and relation types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19292304",
   "metadata": {},
   "source": [
    "The source csv files contain the triples, plus extra information like the types of the entities involved in the triple. We will extract the type information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1878637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(osp.join(DATA_PATH, f'processed/{DATASET}-train.tsv'), sep='\\t', dtype=str)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d0a69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_types_to_entities_dict(df):\n",
    "    \"\"\"Given a dataframe of triples, containing types for entities at the\n",
    "    head and tail, extract a dictionary mapping entity types (str) to\n",
    "    a list of entities of that type.\"\"\"\n",
    "    src_df = df[['src', 'src_type']]\n",
    "    tgt_df = df[['tgt', 'tgt_type']]\n",
    "\n",
    "    src_df = src_df.rename(columns={'src': 'entity', 'src_type': 'type'})\n",
    "    tgt_df = tgt_df.rename(columns={'tgt': 'entity', 'tgt_type': 'type'})\n",
    "    combined_df = pd.concat([src_df, tgt_df]).drop_duplicates(subset='entity')\n",
    "\n",
    "    type_to_entities = combined_df.groupby('type')['entity'].apply(list).to_dict()\n",
    "\n",
    "    return type_to_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3594ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_to_entities = get_types_to_entities_dict(train_df)\n",
    "for t, entities in type_to_entities.items():\n",
    "    print(f'{t}: {len(entities):,} entities')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295c26d",
   "metadata": {},
   "source": [
    "We can now get a list of e.g. Diseases with this dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9e10a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_to_entities['Disease'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e4879a",
   "metadata": {},
   "source": [
    "We can now run the evaluation by relation type, separately for heads and tails of a specific type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a01a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_side_prediction_results(model, evaluator, mapped_triples, restrict_entities_to, side: str, relation: str):\n",
    "        assert side in {'head', 'tail'}\n",
    "\n",
    "        results = evaluator.evaluate(model, mapped_triples, restrict_entities_to=restrict_entities_to,\n",
    "                                     do_time_consuming_checks=False, use_tqdm=False)\n",
    "        results_df = results.to_df()\n",
    "        results_df = results_df.loc[(results_df['Side'] == side) & (results_df['Type'] == 'avg')]\n",
    "\n",
    "        results_dict = {'Relation': relation, 'Side': side}\n",
    "        results_dict.update({metric: value for metric, value in zip(results_df['Metric'].values, results_df['Value'].values)})\n",
    "\n",
    "        return results_dict\n",
    "\n",
    "results = []\n",
    "evaluator = RankBasedEvaluator()\n",
    "for relation in tqdm(model.triples_factory.relation_to_id, desc='Evaluating over each relation'):\n",
    "    relation_parts = relation.split('_')\n",
    "    head_type, tail_type = relation_parts[-2:]\n",
    "\n",
    "    # Create subsets based on entity and relation\n",
    "    triples_subset = valid.new_with_restriction(relations=[relation])\n",
    "    if triples_subset.num_triples == 0:\n",
    "        continue\n",
    "\n",
    "    head_entities = type_to_entities[head_type]\n",
    "    tail_entities = type_to_entities[tail_type]\n",
    "    head_ids = torch.tensor(train.entities_to_ids(head_entities), dtype=torch.long)\n",
    "    tail_ids = torch.tensor(train.entities_to_ids(tail_entities), dtype=torch.long)\n",
    "\n",
    "    head_prediction_results = get_side_prediction_results(model, evaluator, triples_subset.mapped_triples, restrict_entities_to=head_ids, side='head', relation=relation)\n",
    "    tail_prediction_results = get_side_prediction_results(model, evaluator, triples_subset.mapped_triples, restrict_entities_to=tail_ids, side='tail', relation=relation)\n",
    "    results.extend([head_prediction_results, tail_prediction_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82001c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_restrict_results_df = pd.DataFrame(results)\n",
    "entity_restrict_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76555f6f",
   "metadata": {},
   "source": [
    "We can check the performance when predicting separately the head and the tail, for each relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7464f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_side_labeled = entity_restrict_results_df.copy()\n",
    "results_df_side_labeled['Relation'] = results_df_side_labeled['Relation'] + '_' + results_df_side_labeled['Side']\n",
    "per_relation_plot(results_df_side_labeled, 'hits_at_10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b05801c",
   "metadata": {},
   "source": [
    "We then average the prediction for the head and the tail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b24c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_both_df = entity_restrict_results_df.groupby('Relation')[entity_restrict_results_df.columns[2:]].mean().reset_index()\n",
    "results_both_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ecd1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_relation_plot(results_both_df, 'hits_at_10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1e697b",
   "metadata": {},
   "source": [
    "Lastly, the overall average is computed over all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e569192",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_both_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c57fe8c",
   "metadata": {},
   "source": [
    "Compare with the results when restricting by relation type only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73a73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "restricted_rels_macro_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91523926",
   "metadata": {},
   "source": [
    "Note that this is **not** the same as the original averages computed without restrictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6fb148",
   "metadata": {},
   "outputs": [],
   "source": [
    "restricted_rels_micro_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f25d58",
   "metadata": {},
   "source": [
    "The micro-average is instead the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f085f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_both_df[results_both_df.columns[2:]].mul(results_df['Count'], axis=0).sum(axis=0) / results_df['Count'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df30d6",
   "metadata": {},
   "source": [
    "We can see that restricting predictions over the correct domain and range of a relation only slightly increases the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
